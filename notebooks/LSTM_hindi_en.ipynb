{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deepanshu/LSTM/env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = open('train_temp.txt').read().split('\\n')\n",
    "random.shuffle(dataset)\n",
    "eng, hin = list(), list()\n",
    "eng_characters, hin_characters = set(), set()\n",
    "\n",
    "for tup in dataset:\n",
    "    if tup == '':\n",
    "        continue\n",
    "    \n",
    "    eng_and_hindi = tup.split('\\t')\n",
    "    \n",
    "    if eng_and_hindi[0].strip() == 'डॅम्प्सकीबेसेल्सकॅबेट':\n",
    "        continue\n",
    "    \n",
    "    eng.append(eng_and_hindi[0])\n",
    "    hin.append(eng_and_hindi[1] + '\\n')\n",
    "    \n",
    "    for char in eng_and_hindi[0]:\n",
    "        if char not in eng_characters:\n",
    "            eng_characters.add(char)\n",
    "            \n",
    "    for char in eng_and_hindi[1]:\n",
    "        if char not in hin_characters:\n",
    "            hin_characters.add(char)\n",
    "            \n",
    "hin_characters.add('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(eng_characters))\n",
    "target_characters = sorted(list(hin_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(word) for word in eng])\n",
    "max_decoder_seq_length = max([len(word) for word in hin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 107206 107206\n",
      "Number of unique input tokens: 61\n",
      "Number of unique output tokens: 107\n",
      "Max sequence length for inputs: 24\n",
      "Max sequence length for outputs: 27\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(eng), len(hin))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict( [(char, i) for i, char in enumerate(input_characters)] )\n",
    "target_token_index = dict( [(char, i) for i, char in enumerate(target_characters)] )\n",
    "input_index_token = dict( [(i, char) for i, char in enumerate(input_characters)] )\n",
    "target_index_token = dict( [(i, char) for i, char in enumerate(target_characters)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(512, input_shape=(max_encoder_seq_length, num_encoder_tokens)))\n",
    "model.add(RepeatVector(max_decoder_seq_length))\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(num_decoder_tokens, activation='softmax')))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(inp):\n",
    "    encoder_input_data = np.zeros((len(inp), max_encoder_seq_length, num_encoder_tokens), \n",
    "                              dtype='float32')\n",
    "    \n",
    "    for i, input_text in enumerate(inp):\n",
    "        for t, char in enumerate(input_text):\n",
    "            encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    \n",
    "    return encoder_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(inp):\n",
    "    decoder_target_data = np.zeros((len(inp), max_decoder_seq_length, num_decoder_tokens),\n",
    "                               dtype='float32')\n",
    "    \n",
    "    for i, target_text in enumerate(inp):\n",
    "        for t, char in enumerate(target_text):\n",
    "            decoder_target_data[i, t, target_token_index[char]] = 1.\n",
    "            \n",
    "    return decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = encode(eng)\n",
    "decoder_target_data = decoder(hin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = encoder_input_data[0:80000, :, :], decoder_target_data[0:80000, :, :]\n",
    "testX, testY = encoder_input_data[80000:, :, :], decoder_target_data[80000:, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 27437 samples\n",
      "Epoch 1/15\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.58171, saving model to model.h5\n",
      " - 139s - loss: 0.5880 - val_loss: 0.5817\n",
      "Epoch 2/15\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.58171 to 0.44805, saving model to model.h5\n",
      " - 137s - loss: 0.5393 - val_loss: 0.4481\n",
      "Epoch 3/15\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.44805 to 0.31281, saving model to model.h5\n",
      " - 135s - loss: 0.3864 - val_loss: 0.3128\n",
      "Epoch 4/15\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.31281 to 0.23194, saving model to model.h5\n",
      " - 135s - loss: 0.2732 - val_loss: 0.2319\n",
      "Epoch 5/15\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.23194 to 0.18166, saving model to model.h5\n",
      " - 135s - loss: 0.2041 - val_loss: 0.1817\n",
      "Epoch 6/15\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.18166 to 0.15553, saving model to model.h5\n",
      " - 135s - loss: 0.1630 - val_loss: 0.1555\n",
      "Epoch 7/15\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.15553 to 0.14196, saving model to model.h5\n",
      " - 135s - loss: 0.1392 - val_loss: 0.1420\n",
      "Epoch 8/15\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.14196 to 0.13297, saving model to model.h5\n",
      " - 135s - loss: 0.1231 - val_loss: 0.1330\n",
      "Epoch 9/15\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.13297 to 0.12724, saving model to model.h5\n",
      " - 135s - loss: 0.1113 - val_loss: 0.1272\n",
      "Epoch 10/15\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.12724 to 0.12546, saving model to model.h5\n",
      " - 135s - loss: 0.1009 - val_loss: 0.1255\n",
      "Epoch 11/15\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.12546 to 0.12306, saving model to model.h5\n",
      " - 135s - loss: 0.0921 - val_loss: 0.1231\n",
      "Epoch 12/15\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 135s - loss: 0.0970 - val_loss: 0.1259\n",
      "Epoch 13/15\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.12306 to 0.12077, saving model to model.h5\n",
      " - 135s - loss: 0.1001 - val_loss: 0.1208\n",
      "Epoch 14/15\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 135s - loss: 0.0830 - val_loss: 0.1299\n",
      "Epoch 15/15\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 135s - loss: 0.0845 - val_loss: 0.1320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc5b8d422b0>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, epochs=15, batch_size=64, \n",
    "          validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(model, source):\n",
    "    prediction = model.predict(source, verbose=0)\n",
    "    integers = [np.argmax(vector) for vector in prediction[0]]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = target_index_token[i]\n",
    "        if word == '\\n':\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target), target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('प स ् ् ि क', ['प', 'स', '्', '्', 'ि', 'क'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model, encode(['publics'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('म ो ब ा इ ल', ['म', 'ो', 'ब', 'ा', 'इ', 'ल'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model, encode(['mobile'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('आ ं स ु ल', ['आ', 'ं', 'स', 'ु', 'ल'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model, encode(['anshul'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('अ ं ु र ा', ['अ', 'ं', 'ु', 'र', 'ा'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model, encode(['anurag'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('अ ं ु र ा', ['अ', 'ं', 'ु', 'र', 'ा'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model, encode(['anubha'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('आ ँ ा क न', ['आ', 'ँ', 'ा', 'क', 'न'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model, encode(['aakansha'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('अ क ा क क श', ['अ', 'क', 'ा', 'क', 'क', 'श'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model, encode(['akanksha'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('आ य ु ष ् न', ['आ', 'य', 'ु', 'ष', '्', 'न'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model, encode(['ayush'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('द ी ं द ु स ् स', ['द', 'ी', 'ं', 'द', 'ु', 'स', '्', 'स'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model, encode(['deepanshu'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('र ं ं द द', ['र', 'ं', 'ं', 'द', 'द'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model, encode(['randy'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ब ा र ् ल ् ्', ['ब', 'ा', 'र', '्', 'ल', '्', '्'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model, encode(['barcelona'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Bidirectional(LSTM(512), input_shape=(max_encoder_seq_length, num_encoder_tokens)))\n",
    "model2.add(RepeatVector(max_decoder_seq_length))\n",
    "model2.add(LSTM(512, return_sequences=True))\n",
    "model2.add(TimeDistributed(Dense(num_decoder_tokens, activation='softmax')))\n",
    "\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model2.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 27206 samples\n",
      "Epoch 1/15\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13622, saving model to model2.h5\n",
      " - 219s - loss: 0.2588 - val_loss: 0.1362\n",
      "Epoch 2/15\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.13622 to 0.11367, saving model to model2.h5\n",
      " - 198s - loss: 0.1198 - val_loss: 0.1137\n",
      "Epoch 3/15\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.11367 to 0.10258, saving model to model2.h5\n",
      " - 198s - loss: 0.0986 - val_loss: 0.1026\n",
      "Epoch 4/15\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10258 to 0.09651, saving model to model2.h5\n",
      " - 197s - loss: 0.0856 - val_loss: 0.0965\n",
      "Epoch 5/15\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09651 to 0.09601, saving model to model2.h5\n",
      " - 198s - loss: 0.0765 - val_loss: 0.0960\n",
      "Epoch 6/15\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.09601 to 0.09352, saving model to model2.h5\n",
      " - 197s - loss: 0.0681 - val_loss: 0.0935\n",
      "Epoch 7/15\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 197s - loss: 0.0611 - val_loss: 0.0949\n",
      "Epoch 8/15\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 219s - loss: 0.0561 - val_loss: 0.0938\n",
      "Epoch 9/15\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 219s - loss: 0.0505 - val_loss: 0.0963\n",
      "Epoch 10/15\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 220s - loss: 0.0467 - val_loss: 0.0991\n",
      "Epoch 11/15\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 220s - loss: 0.0429 - val_loss: 0.0997\n",
      "Epoch 12/15\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 220s - loss: 0.0402 - val_loss: 0.1003\n",
      "Epoch 13/15\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 220s - loss: 0.0380 - val_loss: 0.1030\n",
      "Epoch 14/15\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 218s - loss: 0.0360 - val_loss: 0.1037\n",
      "Epoch 15/15\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 218s - loss: 0.0346 - val_loss: 0.1050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1159cfce10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(trainX, trainY, epochs=15, batch_size=64, \n",
    "          validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('द ी प श ं श', ['द', 'ी', 'प', 'श', 'ं', 'श'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model2, encode(['deepanshu'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('प ब ् ल ि क ् स', ['प', 'ब', '्', 'ल', 'ि', 'क', '्', 'स'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model2, encode(['publics'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('स च ि न', ['स', 'च', 'ि', 'न'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model2, encode(['sachin'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('इ ं ड ि य ा', ['इ', 'ं', 'ड', 'ि', 'य', 'ा'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model2, encode(['india'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('अ ं श ु ल', ['अ', 'ं', 'श', 'ु', 'ल'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model2, encode(['anshul'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('अ न ु र ा ग', ['अ', 'न', 'ु', 'र', 'ा', 'ग'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model2, encode(['anurag'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('अ न ु ब ा', ['अ', 'न', 'ु', 'ब', 'ा'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model2, encode(['anubha'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('अ क ं ं ् ा', ['अ', 'क', 'ं', 'ं', '्', 'ा'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model2, encode(['akanksha'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Bidirectional(LSTM(512), input_shape=(max_encoder_seq_length, num_encoder_tokens)))\n",
    "model3.add(RepeatVector(max_decoder_seq_length))\n",
    "model3.add(Bidirectional(LSTM(512, return_sequences=True)))\n",
    "model3.add(TimeDistributed(Dense(num_decoder_tokens, activation='softmax')))\n",
    "\n",
    "model3.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "filename = 'model3.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 27206 samples\n",
      "Epoch 1/15\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14292, saving model to model3.h5\n",
      " - 322s - loss: 0.2500 - val_loss: 0.1429\n",
      "Epoch 2/15\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14292 to 0.11478, saving model to model3.h5\n",
      " - 320s - loss: 0.1254 - val_loss: 0.1148\n",
      "Epoch 3/15\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.11478 to 0.10344, saving model to model3.h5\n",
      " - 322s - loss: 0.1022 - val_loss: 0.1034\n",
      "Epoch 4/15\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10344 to 0.09916, saving model to model3.h5\n",
      " - 321s - loss: 0.0887 - val_loss: 0.0992\n",
      "Epoch 5/15\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09916 to 0.09598, saving model to model3.h5\n",
      " - 319s - loss: 0.0791 - val_loss: 0.0960\n",
      "Epoch 6/15\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.09598 to 0.09501, saving model to model3.h5\n",
      " - 319s - loss: 0.0707 - val_loss: 0.0950\n",
      "Epoch 7/15\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.09501 to 0.09419, saving model to model3.h5\n",
      " - 322s - loss: 0.0640 - val_loss: 0.0942\n",
      "Epoch 8/15\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 321s - loss: 0.0574 - val_loss: 0.0951\n",
      "Epoch 9/15\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 321s - loss: 0.0527 - val_loss: 0.0975\n",
      "Epoch 10/15\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 321s - loss: 0.0481 - val_loss: 0.0973\n",
      "Epoch 11/15\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 322s - loss: 0.0446 - val_loss: 0.0994\n",
      "Epoch 12/15\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 321s - loss: 0.0417 - val_loss: 0.1012\n",
      "Epoch 13/15\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 321s - loss: 0.0409 - val_loss: 0.1020\n",
      "Epoch 14/15\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 321s - loss: 0.0358 - val_loss: 0.1034\n",
      "Epoch 15/15\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 321s - loss: 0.0343 - val_loss: 0.1045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1158218438>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(trainX, trainY, epochs=15, batch_size=64, \n",
    "          validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('द ी प ा श ु श', ['द', 'ी', 'प', 'ा', 'श', 'ु', 'श'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model3, encode(['deepanshu'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('अ ं श ु ल', ['अ', 'ं', 'श', 'ु', 'ल'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model3, encode(['Anshul'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('अ ु न ु र ा ग', ['अ', 'ु', 'न', 'ु', 'र', 'ा', 'ग'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model3, encode(['Anurag'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('अ ु भ ह ा ा', ['अ', 'ु', 'भ', 'ह', 'ा', 'ा'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model3, encode(['Anubha'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ल प प ट प', ['ल', 'प', 'प', 'ट', 'प'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model3, encode(['laptop'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('आ क क ं ं ् ष ा', ['आ', 'क', 'क', 'ं', 'ं', '्', 'ष', 'ा'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model3, encode(['akanksha'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Bidirectional(LSTM(512, return_sequences=False), input_shape=(max_encoder_seq_length, num_encoder_tokens)))\n",
    "model4.add(RepeatVector(max_decoder_seq_length))\n",
    "model4.add(LSTM(1024, return_sequences=True))\n",
    "model4.add(Dropout(0.2))\n",
    "model4.add(LSTM(512, return_sequences=True))\n",
    "model4.add(Dropout(0.2))\n",
    "model4.add(TimeDistributed(Dense(num_decoder_tokens, activation='softmax')))\n",
    "\n",
    "model4.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "filename = 'model4.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 27206 samples\n",
      "Epoch 1/12\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.16262, saving model to model4.h5\n",
      " - 382s - loss: 0.3620 - val_loss: 0.1626\n",
      "Epoch 2/12\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.16262 to 0.11648, saving model to model4.h5\n",
      " - 379s - loss: 0.1403 - val_loss: 0.1165\n",
      "Epoch 3/12\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.11648 to 0.10448, saving model to model4.h5\n",
      " - 379s - loss: 0.1099 - val_loss: 0.1045\n",
      "Epoch 4/12\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10448 to 0.09794, saving model to model4.h5\n",
      " - 376s - loss: 0.0943 - val_loss: 0.0979\n",
      "Epoch 5/12\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09794 to 0.09490, saving model to model4.h5\n",
      " - 380s - loss: 0.0837 - val_loss: 0.0949\n",
      "Epoch 6/12\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.09490 to 0.09356, saving model to model4.h5\n",
      " - 377s - loss: 0.0748 - val_loss: 0.0936\n",
      "Epoch 7/12\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 376s - loss: 0.0678 - val_loss: 0.0947\n",
      "Epoch 8/12\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.09356 to 0.09323, saving model to model4.h5\n",
      " - 379s - loss: 0.0621 - val_loss: 0.0932\n",
      "Epoch 9/12\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 377s - loss: 0.0570 - val_loss: 0.0957\n",
      "Epoch 10/12\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 376s - loss: 0.0527 - val_loss: 0.0975\n",
      "Epoch 11/12\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 377s - loss: 0.0493 - val_loss: 0.0981\n",
      "Epoch 12/12\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 376s - loss: 0.0467 - val_loss: 0.0996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f11582180f0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.fit(trainX, trainY, epochs=12, batch_size=64, \n",
    "          validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('द ी प ा न ू', ['द', 'ी', 'प', 'ा', 'न', 'ू'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model4, encode(['deepanshu'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('अ ं श ु ल', ['अ', 'ं', 'श', 'ु', 'ल'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model4, encode(['anshul'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('अ न ु र ा ग', ['अ', 'न', 'ु', 'र', 'ा', 'ग'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model4, encode(['anurag'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('आ य ू ष', ['आ', 'य', 'ू', 'ष'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model4, encode(['ayush'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('अ न ू भ ा', ['अ', 'न', 'ू', 'भ', 'ा'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model4, encode(['anubha'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('अ क ा श ् क ा', ['अ', 'क', 'ा', 'श', '्', 'क', 'ा'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model4, encode(['akanksha'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('आ क ा श क क ा', ['आ', 'क', 'ा', 'श', 'क', 'क', 'ा'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model4, encode(['aakanksha'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('इ ं ड ि य ा', ['इ', 'ं', 'ड', 'ि', 'य', 'ा'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model4, encode(['india'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('प ू ज ा', ['प', 'ू', 'ज', 'ा'])\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence(model4, encode(['pooja'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
